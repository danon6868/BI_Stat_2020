---
title: "Project mouse"
author: "Litvinov Daniil"
date: "14/02/2021"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    number_sections: true
editor_options:
    chunk_output_type: console
---

```{r setup, include=F}
knitr::opts_chunk$set(
	echo = F,
	message = FALSE,
	warning = FALSE,
	include = T
)
library(rgl)
knitr::knit_hooks$set(webgl = hook_webgl)
```

```{r, echo = F, message = F, warning = F, include = F}
if (!require('dplyr')){
  install.packages('dplyr')
}
if (!require('ggplot2')){
  install.packages('ggplot2')
}
if (!require('ggcorrplot')){
  install.packages('ggcorrplot')
}
if (!require('corrplot')){
  install.packages('corrplot')
}
if (!require('car')){
  install.packages('car')
}
if (!require('GGally')){
  install.packages('GGally')
}
if (!require('multcomp')){
  install.packages('multcomp')
}
if (!require('readxl')){
  install.packages('readxl')
}
if (!require('vegan')){
  install.packages('vegan')
}
if (!require('factoextra')){
  install.packages('factoextra')
}
if (!require('scatterplot3d')){
  install.packages('scatterplot3d')
}
if (!require('rgl')){
  install.packages('rgl')
}
if (!require('gridExtra')){
  install.packages('gridExtra')
}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
if (!require('DESeq2')){
  BiocManager::install("DESeq2")
}
if (!require('limma')){
  BiocManager::install("limma")
}
if (!require(rgl)){
  install.packages("rgl")
  }
```

Libraries used:

```{r, echo = T}
library(readxl)
library(ggplot2)
library(dplyr)
library(car)
library(multcomp)
library(GGally)
library(corrplot)
library(ggcorrplot)
library(vegan)
library(factoextra)
library(scatterplot3d)
library(rgl)
library(gridExtra)
library('BiocManager')
library(limma)
library(rgl)
```

```{r, echo = F}
theme_set(theme_bw())
```


# Introduction

In this work I wiil use data from article about the effect of Down syndrome on different proteins production. Data for this project can be found [here](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression#).

*Goal* -- conduct EDA, make a few linear models to predict production level of some proteins and also make PCA.
In additional part of this project I will analyze differential expressing genes.

# Data description

## Data description

```{r}
mouse_data <- read_xls('Data/Data_Cortex_Nuclear.xls')
str(mouse_data)
```

You can see that MouseID contain information both about mouse id and about technical repetition. To create new variable **id** in data I made this function:

```{r, echo = T}
make_id <- function(data){
  f <- function(x){
    list_of_letters <- strsplit(x, '')
    under_index <- which(list_of_letters[[1]] == '_')
    return(substr(x, 1, under_index - 1))
  }
  ids <- unlist(lapply(data, f))
  return(ids)
}
```

This function takes one argument (column) and return vector, consisting of first parts of strings till "_". So we can get information only about mouse id.

```{r}
mouse_data$id <- make_id(mouse_data$MouseID)
```

There are `r length(unique(mouse_data$id))` mice in experiment. It may be a good idea to make some of our variables as factors.

```{r}
mouse_data$Genotype <- as.factor(mouse_data$Genotype)
mouse_data$Treatment <- as.factor(mouse_data$Treatment)
mouse_data$Behavior <- as.factor(mouse_data$Behavior)
mouse_data$class <- as.factor(mouse_data$class)
classes <- levels(mouse_data$class)
```

We can distinguish `r length(classes)` groups by variable **class**. 

```{r}
class_pivot_count <- mouse_data %>%
                      group_by(class) %>% 
                      summarise(count = n() / 15)
treat_pivot_count <- mouse_data %>%
                     group_by(Treatment) %>% 
                     summarise(count = n() / 15)
genotype_pivot_count <- mouse_data %>%
                        group_by(Genotype) %>% 
                        summarise(count = n() / 15)
behavior_pivot_count <- mouse_data %>%
                        group_by(Behavior) %>% 
                        summarise(count = n() / 15)
```

This function helps to create barplots of distribution of mice in groups based on different variables:

```{r, echo = T}
group_distr_col <- function(data, x, y = 'count'){
  ggplot(data, aes(x = list(data[x][[1]])[[1]], y = list(data[y][[1]])[[1]])) +
    geom_col(fill = '#FDC65E') +
    theme(plot.title = element_text(hjust = 0.5)) + 
    xlab(paste0('Groups based on variable ', x)) + 
    ylab('Number of mice') +
    ggtitle(paste0('Distribution of mice\nin groups based on variable ', x)) +
    scale_x_discrete(guide = guide_axis(angle = 30))
}
```

```{r, fig.width = 10, fig.height = 8}
grid.arrange(
            group_distr_col(class_pivot_count, 'class'),
            group_distr_col(behavior_pivot_count, 'Behavior'),
            group_distr_col(treat_pivot_count, 'Treatment'),
            group_distr_col(genotype_pivot_count, 'Genotype'),
            ncol = 2)
```

We can see that the groups are not quite balanced and ANOVA that we plan to apply further may be less resistant to violation of the conditions of applicability.

Classes:

- c-CS-s: control mice, stimulated to learn, injected with saline (9 mice)
- c-CS-m: control mice, stimulated to learn, injected with memantine (10 mice)
- c-SC-s: control mice, not stimulated to learn, injected with saline (9 mice)
- c-SC-m: control mice, not stimulated to learn, injected with memantine (10 mice)
- t-CS-s: trisomy mice, stimulated to learn, injected with saline (7 mice)
- t-CS-m: trisomy mice, stimulated to learn, injected with memantine (9 mice)
- t-SC-s: trisomy mice, not stimulated to learn, injected with saline (9 mice)
- t-SC-m: trisomy mice, not stimulated to learn, injected with memantine (9 mice)

## Making data tidy

Now there is a time to deal with NA. There are `r sum(is.na(mouse_data))` missing values in our dataset. It is interesting to check distribution of NAs in different groups. May be some proteins just were not expressed in certain conditions. 

```{r, echo = F, include = F}
na_data <- apply(apply(mouse_data, 2, is.na), 2, sum)
```

```{r, include = F}
c_CS_m <- mouse_data[mouse_data$class == 'c-CS-m', ][, 2:78]
c_CS_s <- mouse_data[mouse_data$class == 'c-CS-s', ][, 2:78]
c_SC_m <- mouse_data[mouse_data$class == 'c-SC-m', ][, 2:78]
c_SC_s <- mouse_data[mouse_data$class == 'c-SC-s', ][, 2:78]
t_CS_m <- mouse_data[mouse_data$class == 't-CS-m', ][, 2:78]
t_CS_s <- mouse_data[mouse_data$class == 't-CS-s', ][, 2:78]
t_SC_m <- mouse_data[mouse_data$class == 't-SC-m', ][, 2:78]
t_SC_s <- mouse_data[mouse_data$class == 't-SC-s', ][, 2:78]

pivot_na_data <- data.frame(class = classes, 
                            rbind(apply(apply(c_CS_m, 2, is.na), 2, sum),
                                        apply(apply(c_CS_s, 2, is.na), 2, sum),
                                        apply(apply(c_SC_m, 2, is.na), 2, sum),
                                        apply(apply(c_SC_s, 2, is.na), 2, sum),
                                        apply(apply(t_CS_m, 2, is.na), 2, sum),
                                        apply(apply(t_CS_s, 2, is.na), 2, sum),
                                        apply(apply(t_SC_m, 2, is.na), 2, sum),
                                        apply(apply(t_SC_s, 2, is.na), 2, sum)))
```

```{r}
pivot_na_data
```

Since there are quite a few missing values in the data, I will replace them with group means (class variable).

```{r}
mouse_data_wo_na <- mouse_data %>% group_by(class) %>% mutate(across(everything(), function(x) ifelse(is.na(x), mean(x, na.rm = T), x)))
mouse_data_wo_na$Genotype <- mouse_data$Genotype
mouse_data_wo_na$Treatment <- mouse_data$Treatment
mouse_data_wo_na$Behavior <- mouse_data$Behavior
```

# BDNF_N production

I am going to conduct ANOVA so that to find out if the level of BDNF_N production differs in different classes:

```{r}
bdnf_mod <- lm(BDNF_N ~ class, data = mouse_data_wo_na)
av_bdn_mod <- Anova(bdnf_mod)
av_bdn_mod
```

We can see that **class** is a significant predictor. So we can assume that there are some differences in the BDNF_N production level depending on the class in experiment. But first we have to check the conditions of applicability and conduct post-hoc tests.

## Checking the applicability conditions 

```{r}
bdnf_mod_diag <- fortify(bdnf_mod)
```

**Graph of Cook's distances**

There are several heuristics for calculating threshold for Cook's distances. It can be just 2, 3 * $\bar{y}$ or $\frac4n$, where n is number of samples. Here I am going to use the second

```{r}
cook_threshold <- 3 * mean(bdnf_mod_diag$BDNF_N)
```


```{r}
ggplot(bdnf_mod_diag, aes(x = 1:nrow(bdnf_mod_diag), y = .cooksd)) +
  geom_bar(stat = 'identity') +
  geom_hline(yintercept = cook_threshold, color = "red") + 
  xlab("Samle number") + 
  ylab("Cook's distance") +
  ggtitle("Graph of Cook's distances") +
  theme(plot.title = element_text(hjust = 0.5))
```

There are no samples which are bigger than threshold.

**Model residues**

Residuals plots show that there are quite a few poorly predicted values, but no particular pattern is observed, also the median value of residuals in different classes is approximately the same and is close to zero.

```{r}
ggplot(data = bdnf_mod_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0) +
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red") +
  xlab('Prediction') + 
  ylab('Standardized residuals') +
  ggtitle('Distribution of standardized residuals of model\nin fitted values') +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(bdnf_mod_diag, aes(x = class, y = .stdresid)) +
  geom_boxplot() + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  xlab('Class') + 
  ylab('Standardized residuals') +
  ggtitle('Distribution of standardized residuals of model\nin class')
```

**Normal distribution of model residuals**

We can't name the distribution of the residuals of the model a normal, but we may try to apply ANOVA because other conditions generally applicable and also we have lots of samples that is good for ANOVA.

```{r}
qqPlot(bdnf_mod, xlab = 'Normal distribution quantiles', ylab = 'Quantiles of the distribution of the model residuals')
shapiro.test(bdnf_mod_diag$.resid)
```

**Post-hoc tests**

Here I will use Tukey post-hoc test:

```{r}
res_tukey <- glht(bdnf_mod, linfct = mcp(class = 'Tukey'))
summary(res_tukey)
```

**Post-hoc tests visualization**

```{r}
data <-  expand.grid(class = mouse_data_wo_na$class)
data <- data.frame(data,
                   predict(bdnf_mod, newdata = data, interval = 'confidence'))
pos <- position_dodge(width = 0.2)
gg_linep <- ggplot(data, aes(x = class, y = fit,
                             ymin = lwr, ymax = upr)) + 
  geom_point(position = pos) +
  geom_errorbar(position = pos, width = 0.2) +
  theme(plot.title = element_text(hjust = 0.5)) + 
  xlab('Class') + 
  ylab('Predicted mean') +
  ggtitle('Dependence of the predicted\nmean expression of BDNF_N in the class')
gg_linep
```

It can be seen that there is a significant dependence of the BDNF_N production in the class in the experiment.

BDNF belongs to neurotrophins, substances that stimulate and support the development of neurons. BDNF acts on certain neurons in the central and peripheral nervous systems, helping emerging neurons survive, and increases the number and differentiation of new neurons and synapses. Based on the results of post-hoc tests, the following conclusions can be drawn:

1. In the memantine applicable control mice, BDNF production was significantly higher when stimulated to learn. The same effect is observed in the group of saline control mice
2. The learning stimulation causes a significantly higher level of BDNF production in the case of control mice (compared to mice with trisomy)
3. Unstimulated mice with trisomy which were treated with memantine have significantly higher level of BDNF_N production than the same control mice

# ERBB4_N production

```{r, fig.width = 7, fig.height = 7}
exp_data <- mouse_data_wo_na[, -c(1:6)]
ggcorr(exp_data, hjust = 0.75, size = 1.5, color = "black", layout.exp = 1)
```

Judging by this heat-map, we can expect the presence of multicollinearity in the data, but this still has to be checked.
I am going to build a complete linear model for the ERBB4_N protein.

```{r}
erbb_mod <- lm(ERBB4_N ~ ., data = exp_data)
```

## Model diagnostics

```{r}
erbb_mod_diag <- fortify(erbb_mod)
```

```{r, include = F}
erbb_threshold <- 3 * mean(erbb_mod_diag$ERBB4_N)
```

**Graph of Cook's distances**

There are no influential observations.

```{r}
ggplot(erbb_mod_diag, aes(x = 1:nrow(erbb_mod_diag), y = .cooksd)) +
  geom_bar(stat = 'identity') +
  xlab('Samle number') + 
  ylab("Cook's distance") +
  ggtitle("Graph of Cook's distances") +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_hline(yintercept = erbb_threshold, color = "red")
```

**Model residues**

The residuals have a pronounced pattern and many observations are lagging behind by more than 2 standard deviations.

```{r}
ggplot(data = erbb_mod_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0) +
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red") +
  xlab('Prediction') + 
  ylab('Standardized residuals') +
  ggtitle('Distribution of standardized residuals of model\nin fitted values') + 
  theme(plot.title = element_text(hjust = 0.5))
```

**Normal distribution of model residuals**

Distribution of standardized residues differ significantly from normal. 

```{r}
qqPlot(erbb_mod)
shapiro.test(erbb_mod_diag$.stdresid)
```

**Checking for multicollinearity**

Multicollinearity is likely to exist in our data, this is also clear from the point of view of biology, since many proteins form metabolic pathways, and, accordingly, are expressed together.

```{r, include = F}
alias(erbb_mod)
```

The data showed the effect of complete collinearity. In this case, the VIF function will not work without giving an additional parameter. In this case, you can see which variable has this effect and remove it from the model. Here it is ARC_N but even so, the VIF of most variables is very high. We can immediately conclude that such a model is a bad decision:.

Many conditions for the applicability of linear models are violated, including multicollinearity, which can, of course, be eliminated by removing predictors with high VIF, but you will have to remove a lot and it is not a fact that the model will be good with a small number of predictors. In this case, we can try using dimension reduction methods.

# PCA

## Calculation of principal components

Let's make a dataframe in which each observation is the average protein expression for each mouse over replicates.

```{r}
mouse_wo_reps <- mouse_data_wo_na[, -c(1, 79, 80, 81, 82)] %>% group_by(id) %>% summarise_all('mean')
mouse_wo_reps <- arrange(mouse_wo_reps, id)
cl <- arrange(mouse_data_wo_na[!duplicated(mouse_data_wo_na$id), ], id)$class
gn <- arrange(mouse_data_wo_na[!duplicated(mouse_data_wo_na$id), ], id)$Genotype
tr <- arrange(mouse_data_wo_na[!duplicated(mouse_data_wo_na$id), ], id)$Treatment
bh <- arrange(mouse_data_wo_na[!duplicated(mouse_data_wo_na$id), ], id)$Behavior

mouse_wo_reps$class <- cl
mouse_wo_reps$Genotype <- gn
mouse_wo_reps$Treatment <- tr
mouse_wo_reps$Behavior <- bh
```

```{r}
mouse_wo_reps_pca <- rda(mouse_wo_reps[, -c(1, 79, 80, 81, 82)], scale = T)
```

## Graphs

**Factor loadings**

The angles between the vectors reflect the correlations of features with each other and with the axes of the principal components. With a large number of variables, it is quite difficult to interpret this graph, but still it can be seen that most of the variables have a positive correlation with PC1.

```{r}
biplot(mouse_wo_reps_pca, scaling = 'species', display = 'species', main = 'Correlation biplot (loads graph)')
```

**Distance biplot**

Distances between points (objects - mice) approximate Euclidean distances and reflect the similarity between objects - mice.

```{r}
biplot(mouse_wo_reps_pca, scaling = 'sites', display = 'sites', main = 'Distance biplot (ordination plot)', type = 'points')
```

**Ordination plot using ggplot**

Graph of ordination in the axes of the first two principal components:

```{r, fig.width = 12, fig.height = 12}
df_scores <- data.frame(mouse_wo_reps,
                        scores(mouse_wo_reps_pca, display = "sites", choices = c(1, 2, 3), scaling = "sites"))
cl <- ggplot(df_scores, aes(x = PC1, y = PC2)) + 
      geom_point(aes(color = class), alpha = 0.5) +
      coord_equal(xlim = c(-1.2, 1.2), ylim = c(-1.2, 1.2)) + ggtitle(label = "Principal component axis ordination") + theme_bw() +
      theme(plot.title = element_text(hjust = 0.5))
fst <- ggplot(df_scores, aes(x = PC1, y = PC2)) + 
      geom_point(aes(color = Treatment), alpha = 0.5) +
      coord_equal(xlim = c(-1.2, 1.2), ylim = c(-1.2, 1.2)) + ggtitle(label = "Principal component axis ordination") + theme_bw() +
      theme(plot.title = element_text(hjust = 0.5))
sec <- ggplot(df_scores, aes(x = PC1, y = PC2)) + 
      geom_point(aes(color = Genotype), alpha = 0.5) +
      coord_equal(xlim = c(-1.2, 1.2), ylim = c(-1.2, 1.2)) + ggtitle(label = "Principal component axis ordination") + theme_bw() +
      theme(plot.title = element_text(hjust = 0.5))
thr <- ggplot(df_scores, aes(x = PC1, y = PC2)) + 
      geom_point(aes(color = Behavior), alpha = 0.5) +
      coord_equal(xlim = c(-1.2, 1.2), ylim = c(-1.2, 1.2)) + ggtitle(label = "Principal component axis ordination") + theme_bw() +
      theme(plot.title = element_text(hjust = 0.5))

grid.arrange(cl, fst, sec, thr, ncol = 2)
```

Interestingly, according to the Genotype and Treatment variables, mice are poorly separated in the space of the first two components, and according to the Behavior variable, yes, learning probably has the strongest effect on protein production in the data.

Graph of ordination in the axes of the first three principal components:

```{r, webgl=TRUE, echo=FALSE, fig.height=10, fig.width=10, include = T}
# It is a little bit difficult to create legend on interactive plot :(

par(mar = c(0, 0, 0, 0))

mycolors <- c('#05173e', '#eed8ae', '#f6b969', '#93b8d3', '#759194', '#037272', '#ce073f', '#d8d4ea', '#cbdbcd')

df_scores$color <- mycolors[as.numeric(df_scores$class) ]
plot3d(
    x = df_scores$PC1, df_scores$PC2, df_scores$PC3,
    col = df_scores$color,
    type = "s",
    radius = .05,
    xlab = "PC1", ylab = "PC2", zlab = "PC3", main = "Observations colored by mice class")
```

```{r, webgl=TRUE, echo=FALSE, fig.height=10, fig.width=10, include = T}
df_scores$color <- mycolors[as.numeric(df_scores$Behavior) ]
plot3d(
    x = df_scores$PC1, df_scores$PC2, df_scores$PC3,
    col = df_scores$color,
    type = "s",
    radius = .05,
    xlab = "PC1", ylab = "PC2", zlab = "PC3", main = "Observations colored by mice Behavior")
```

```{r, webgl=TRUE, echo=FALSE, fig.height=10, fig.width=10, include = T}
df_scores$color <- mycolors[as.numeric(df_scores$Treatment) ]
plot3d(
    x = df_scores$PC1, df_scores$PC2, df_scores$PC3,
    col = df_scores$color,
    type = "s",
    radius = .05,
    xlab = "PC1", ylab = "PC2", zlab = "PC3", main = "Observations colored by mice Treatment")
```

```{r, webgl=TRUE, echo=FALSE, fig.height=10, fig.width=10, include = T}
df_scores$color <- mycolors[as.numeric(df_scores$Genotype) ]
plot3d(
    x = df_scores$PC1, df_scores$PC2, df_scores$PC3,
    col = df_scores$color,
    type = "s",
    radius = .05,
    xlab = "PC1", ylab = "PC2", zlab = "PC3", main = "Observations colored by mice Genotype")
```

## PCA interpretation

### Number of components

At the very beginning, you need to understand how each component contributes. 

```{r}
screeplot(mouse_wo_reps_pca, type = "lines", bstick = F, main = 'Eigenvalues plot')
```

Another good way is to see what percentage of variance each component explains. It is usually sufficient that the total percentage of variability of the components is about 90 %. It can be seen from the summary model that for this it is necessary to use 14 principal components.

```{r}
pca_summary <- summary(mouse_wo_reps_pca)
pca_result <- as.data.frame(pca_summary$cont)
pca_result <- pca_result[, 1:9]
plot_data <- as.data.frame(t(pca_result[c("Proportion Explained"),]))
plot_data$component <- rownames(plot_data)
plot_data$component <- as.factor(plot_data$component)
ggplot(plot_data, aes(component, y = `Proportion Explained`)) + 
  geom_bar(stat = "identity") + 
  theme_bw() + 
  ggtitle(label = "Explained variance") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_discrete(guide = guide_axis(angle = 30))
  
```

# ERBB4_N production via PCA

Build a new model using 14 first PC.

```{r}
mouse_wo_erbb <- mouse_wo_reps[, -c(1, 56, 79, 80, 81, 82)]
mouse_erbb_pca <- rda(mouse_wo_erbb, scale = T)
mouse_for_model <- data.frame(ERBB4_N = mouse_wo_reps$ERBB4_N, scores(mouse_wo_reps_pca, display = "sites", choices = 1:14, scaling = "sites"))
model_via_pca <- lm(ERBB4_N ~ ., data = mouse_for_model)
summary(model_via_pca)
```

## Model diagnostics

```{r}
erbb_mod_diag <- fortify(model_via_pca)
```

```{r, include = F}
erbb_threshold <- 3 * mean(erbb_mod_diag$ERBB4_N)
```


**Graph of Cook's distances**

Here we can conclude that there is no influential observations in this case.

```{r}
ggplot(erbb_mod_diag, aes(x = 1:nrow(erbb_mod_diag), y = .cooksd)) +
  geom_bar(stat = 'identity') +
  xlab('Samle number') + 
  ylab("Cook's distance") +
  ggtitle("Graph of Cook's distances") +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_hline(yintercept = erbb_threshold, color = "red")
```

**Model residues**

Distribution of standardized residuals of model also looks good.

```{r}
ggplot(data = erbb_mod_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0) +
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red") +
  xlab('Prediction') + 
  ylab('Standardized residuals') +
  ggtitle('Distribution of standardized residuals of model\nin fitted values') + 
  theme(plot.title = element_text(hjust = 0.5))
```

**Normal distribution of model residuals**

Distribution of standardized residues does not differ significantly from a normal. 

```{r}
qqPlot(erbb_mod)
shapiro.test(erbb_mod_diag$.stdresid)
```

In general, it is clear that this model is already quite good. It would also be possible to select only significant predictors using a partial F-test, but since we will not predict anything further, this will remain outside the scope of this work.

# Search for differentially expressed proteins

In this work I will use *limma*. Limma is an R package that was originally developed for differential expression (DE) analysis of microarray data. It fits a linear model to each row to find differentially expressed proteins.

```{r}
mouse_transposed <- data.frame(t(mouse_wo_reps[, -c(1, 79:82)]))
design_beh <- model.matrix(~ mouse_wo_reps$Behavior)
design_treat <- model.matrix(~ mouse_wo_reps$Treatment)
design_gt <- model.matrix(~ mouse_wo_reps$Genotype)

mouse_beh_fit <- lmFit(mouse_transposed, design_beh)
mouse_treat_fit <- lmFit(mouse_transposed, design_treat)
mouse_gt_fit <- lmFit(mouse_transposed, design_gt)

mouse_beh_fit <- eBayes(mouse_beh_fit)
mouse_treat_fit <- eBayes(mouse_treat_fit)
mouse_gt_fit <- eBayes(mouse_gt_fit)

gene_beh_list <- topTable(mouse_beh_fit, n = 77)
gene_treat_list <- topTable(mouse_treat_fit, n = 77)
gene_gt_list <- topTable(mouse_gt_fit, n = 77)

de_beh_result <- filter(gene_beh_list, adj.P.Val <= 0.05)
de_treat_result <- filter(gene_treat_list, adj.P.Val <= 0.05)
de_gt_result <- filter(gene_gt_list, adj.P.Val <= 0.05)

data.frame(Variable = c('Behavior', 'Treatment', 'Genotype'),
           Number_of_DE_genes = c(nrow(de_beh_result),
                                  nrow(de_treat_result),
                                  nrow(de_gt_result)))
```

It can be seen that there are almost no differentially expressed proteins for the Treatment and Genotype variables, while there are 47 of them for the Behavior variable. This generally confirms the results of PCA described above. Annotation of the list of these proteins is outside the scope of this project, but [here](https://reactome.org/content/query?q=SOD1+CaNA+P38+Ubiquitin+SNCA+ARC+pS6+pERK+EGR1+H3MeK4+pPKCAB+GSK3B+pGSK3B+DYRK1A+ITSN1+pCAMKII+IL1B+BRAF+pMTOR+pNR2A+ERK+nNOS+S6+H3AcK18+pMEK+pAKT+pBRAF+CDK5+PSD95+pP70S6+NR2B+MTOR+AKT+pNUMB+pJNK+ERBB&species=Mus+musculus&types=Pathway&cluster=true) you can find metabolic pathways that include differentially expressed proteins (by Behavior variable).
